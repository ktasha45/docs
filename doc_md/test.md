# ai/ml
## ai/ml
* [Escaping the Gradient Vanishing: Periodic Alternatives of Softmax in Attention Mechanism](https://arxiv.org/abs/2108.07153)
    * 16 Aug 2021  
    * The statistical assumption that the input is **normal distributed** supports the gradient stability of Softmax.  
 -> normal distribution과 softmax의 관계??
 - [iid 관련](https://www.samsungsds.com/kr/insights/data_science.html)
 - [GELUs](https://hongl.tistory.com/236)
   - GELU에 대한 설명
   - Gaussian Error Linear Units
   - [paper](https://arxiv.org/abs/1606.08415)
     - 27 Jun 2016
 - [numerical stability](https://ko.d2l.ai/chapter_deep-learning-basics/numerical-stability-and-init.html)
   - d2l.ai (번역본)
   - 그래디언트 소실(vanishing)과 폭발(exploding)
 - [Inductive bias](https://mino-park7.github.io/nlp/2019/03/18/emnlp-2018-%EC%A3%BC%EC%9A%94-%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC/)