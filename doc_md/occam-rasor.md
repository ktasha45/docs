# occam's razor

1. "많은 것들을 필요없이 가정해서는 안된다" (*Pluralitas non est ponenda sine neccesitate.*)
2. "더 적은 수의 논리로 설명이 가능한 경우, 많은 수의 논리를 세우지 말라." (*Frustra fit per plura quod potest fieri per pauciora.*)

사고 절약의 원리(*principle of parsimony*)라고도 불린다. '설명은 단순할수록 뛰어나다', '불필요한 가정을 늘리지 마라' 등의 의미로 해석되고 있다.  

지구 밖에 생명체가 고대 지구인에게 예술을 가르쳐주었을 지도 모른다. 그러나, 고대인의 기술과 에술을 설명하는 데에, 우주인의 방문을 가정할 필요는 없다. 왜 불필요한 가정을 만드는가? 

* wikipedia
> 이는 논리학에서의 "추론의 건전성" 개념과도 비슷한 면이 있다. 논리학에서는 추론이 타당한 것으로 밝혀지면 추론의 건전성을 검사하는데, 타당한 추론이라면 결론이 정당화될 수 있는 정도는 그 추론에서 가장 정당하지 못한 전제가 정당화되는 정도를 넘지 못한다. 따라서 논리의 형식상으로는 타당한 논증이라고 해도, 논증에 가정이 많이 들어가면 들어갈수록 그 논증이 건전하지 못한 논증이 될 가능성도 높아지는 것이고, 이를 바꿔 말하면 가능한 한 가정이 적게 포함된 논증일수록 더욱더 건전할 가능성이 높다고 할 수 있는 것이다.

* [머신러닝과 오컴의 면도날(occam's razor) 법칙](https://www.youtube.com/watch?v=iWtdGpSYEC0)
> 머신러닝에서도 중요한 주제이다. 그 여러 방법들 중 비슷한 성능을 보이는 애들이 있다고 한다면, 어떤걸 선택해야 하는가?
가장 높은 걸 선택하는 게 가장 현명해 보인다. 당연히 단기적으로 보면 정확도가 우수한 것이 가장 좋다. 하지만, 예측 모델의 제작은, 한번 하고 끝나는 것이 아니다. 모델의 개발은 반복되는 과정이고 그 과정에서 domain knowledge가 들어가고 데이터가 들어가고, 또 계속해서 데이터가 증가하거나 feature가 증가한다면? 가급적 간단하고 쉬운 걸 쓰는 게 좋다. 그래야 모델을 개선하기 쉽고, 추가적인 구현을 더 자유롭게 할 수 있다.따라서, 되도록 쉽고 간단한 걸 선택하는 것이 더 좋은 결과를 가져올 수 있다.  
\+ 결국 알고리즘은 그렇게 중요한 것이 아니다. 데이터를 잘 분석하고 다양한 알고리즘의 특성을 이해하고 잘 활용하는 것이 훨씬 더 중요하다.



---
## **reference**
* [aistudy - Occam's Razor](http://www.aistudy.co.kr/heuristic/occams_razor.htm)
* [wikipedia](https://ko.wikipedia.org/wiki/%EC%98%A4%EC%BB%B4%EC%9D%98_%EB%A9%B4%EB%8F%84%EB%82%A0)