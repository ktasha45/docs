<h1>ai/ml</h1>
<h2>ai/ml</h2>
<ul>
<li><a href="https://arxiv.org/abs/2108.07153">Escaping the Gradient Vanishing: Periodic Alternatives of Softmax in Attention Mechanism</a><ul>
<li>16 Aug 2021  </li>
<li>The statistical assumption that the input is <strong>normal distributed</strong> supports the gradient stability of Softmax.<br />
 -&gt; normal distribution과 softmax의 관계??</li>
</ul>
</li>
<li><a href="https://www.samsungsds.com/kr/insights/data_science.html">iid 관련</a></li>
<li><a href="https://hongl.tistory.com/236">GELUs</a></li>
<li>GELU에 대한 설명</li>
<li>Gaussian Error Linear Units</li>
<li><a href="https://arxiv.org/abs/1606.08415">paper</a><ul>
<li>27 Jun 2016</li>
</ul>
</li>
<li><a href="https://ko.d2l.ai/chapter_deep-learning-basics/numerical-stability-and-init.html">numerical stability</a></li>
<li>d2l.ai (번역본)</li>
<li>그래디언트 소실(vanishing)과 폭발(exploding)</li>
<li><a href="https://mino-park7.github.io/nlp/2019/03/18/emnlp-2018-%EC%A3%BC%EC%9A%94-%EB%85%BC%EB%AC%B8-%EC%A0%95%EB%A6%AC/">Inductive bias</a></li>
</ul>